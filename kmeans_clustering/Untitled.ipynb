{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import random as rand;\n",
    "import math;\n",
    "import copy;\n",
    "import numpy as np;\n",
    "import itertools;\n",
    "\n",
    "## mlCons / dlCons structure: [(instance, instance), ... (instance, instance)]\n",
    "## instance / point structure: Set(attr1, attr2...)\n",
    "class ConstrainedKMeans:\n",
    "    def __init__(self, clustersQty, convergeThreshold, distFunction):\n",
    "        self.clustersQty = clustersQty;\n",
    "        self.convergeThreshold = convergeThreshold;\n",
    "        self.distFunction = distFunction;\n",
    "\n",
    "    #This functiion trains with the dataset.\n",
    "    def clusterize(self, dataset, mlCons, dlCons):\n",
    "        print('clusterizing with ', self.clustersQty, \" clusters...\");\n",
    "\n",
    "        self.clusters = self.__getInitialClusters(dataset.copy());\n",
    "        self.oldClusters = None;\n",
    "        \n",
    "        while (not self.__converged()):\n",
    "            self.clusterPoints = {k : [] for k in self.clusters.keys()};\n",
    "            self.noCluster = [];\n",
    "            self.__assignPoints(dataset, mlCons, dlCons);\n",
    "            self.oldClusters = copy.deepcopy(self.clusters);\n",
    "            self.__updateClusters();\n",
    "\n",
    "        # print('Cluster x Points: ', self.clusterPoints);\n",
    "        # print('Clusters: ', self.clusters);\n",
    "        return self.clusterPoints;\n",
    "\n",
    "    #This function shall check if the function has stop converging (we should limit a threshold)\n",
    "    def __converged(self):\n",
    "        if (self.oldClusters != None):\n",
    "            for i in self.oldClusters.keys():\n",
    "                if (abs(np.std(self.oldClusters[i]) - np.std(self.clusters[i])) > self.convergeThreshold):\n",
    "                    print('CONVERGE MORE!')\n",
    "                    return False;\n",
    "        else:\n",
    "            return False;\n",
    "\n",
    "        return True;\n",
    "\n",
    "    #This function shall assign the points to the clusters according to its distance from the clusters.\n",
    "    def __assignPoints(self, dataset, mlCons, dlCons):\n",
    "        for point in dataset:\n",
    "            ##TODO check if should insert the points with constraints first.\n",
    "            cluster = self.__findNearestCluster(point);\n",
    "            if (not self.__violateConstraints(point, cluster, mlCons, dlCons)):\n",
    "                self.clusterPoints[cluster].append(point);\n",
    "            else:\n",
    "                self.noCluster.append(point);\n",
    "\n",
    "    def __findNearestCluster(self, point):\n",
    "        choosenCluster = None;\n",
    "        choosenDist = None;\n",
    "        for c in self.clusters.items():\n",
    "            if (choosenCluster == None):\n",
    "                choosenCluster = c[0];\n",
    "                choosenDist = self.distFunction.getDist(point, c[1]);\n",
    "            elif (self.distFunction.getDist(point, c[1]) < choosenDist):\n",
    "                choosenCluster = c[0];\n",
    "                choosenDist = self.distFunction.getDist(point, c[1]);\n",
    "\n",
    "        return choosenCluster;\n",
    "\n",
    "\n",
    "    #This function shall move the clusters according to its points' positions.\n",
    "    def __updateClusters(self):\n",
    "        for cp in self.clusterPoints.keys():\n",
    "            for attr in range(0, self.clusters[cp].size):\n",
    "                self.clusters[cp][attr] = sum(x[attr] for x in self.clusterPoints[cp])/len(self.clusterPoints[cp]);\n",
    "\n",
    "    #This function gets the initial clusters, avoiding to get the same cluster point at the same time.\n",
    "    ##TODO do this better.\n",
    "    def __getInitialClusters(self, dataset):\n",
    "        if (np.unique(dataset).size < self.clustersQty):\n",
    "            raise ValueError('O número de instâncias únicas do dataset deve ser maior ou igual o número de grupos.');\n",
    "\n",
    "        keepChoosingPoints = True;\n",
    "        while (keepChoosingPoints):\n",
    "            cls = {k : rand.choice(dataset) for k in range(self.clustersQty)};\n",
    "            aux = set([tuple(cl) for cl in cls.values()]);\n",
    "\n",
    "            if (self.clustersQty == len(aux)):\n",
    "                keepChoosingPoints = False;\n",
    "\n",
    "        return cls;\n",
    "\n",
    "    #This function is the article's violate-contraint function.\n",
    "    def __violateConstraints(self, point, cluster, mlCons, dlCons):\n",
    "        mustLink = [x for x in mlCons if (any((point == y).all() for y in x))];\n",
    "\n",
    "        if (len(mustLink) > 0):\n",
    "            for ml in mustLink:\n",
    "                if ((point == ml[0]).all()):\n",
    "                    pairCluster = self.__findNearestCluster(ml[1]);\n",
    "                else:\n",
    "                    pairCluster = self.__findNearestCluster(ml[0]);\n",
    "                if (pairCluster != cluster):\n",
    "                    return True;\n",
    "\n",
    "        dontLink = [x for x in dlCons if (any((point == y).all() for y in x))];\n",
    "\n",
    "        if (len(dontLink) > 0):\n",
    "            for dl in dontLink:\n",
    "                if ((point == dl[0]).all()):\n",
    "                    pairCluster = self.__findNearestCluster(dl[1]);\n",
    "                else:\n",
    "                    pairCluster = self.__findNearestCluster(dl[0]);\n",
    "                if (pairCluster == cluster):\n",
    "                    return True;\n",
    "\n",
    "        return False;\n",
    "\n",
    "\n",
    "class DistanceMetrics:\n",
    "\n",
    "    class EuclidianDistance:\n",
    "        def getDist(self, X, Y):\n",
    "            tuples = zip(X, Y)\n",
    "            distance = 0\n",
    "            for x, y in tuples:\n",
    "                distance += (x - y) ** 2\n",
    "            return math.sqrt(distance)\n",
    "        \n",
    "    class SimpleMatchDistance:\n",
    "        def getDist(self, X, Y):\n",
    "            tuples = zip(X, Y)\n",
    "            distance = 0\n",
    "            for x, y in tuples:\n",
    "                if(x != y):\n",
    "                    distance += 1\n",
    "            return distance\n",
    "\n",
    "class ReportResults:\n",
    "    \n",
    "    def __print_index(self, source, item_to_search):\n",
    "        list_items = list(source)\n",
    "        for i, item in enumerate(list_items):\n",
    "            if((item==item_to_search).all()):\n",
    "                print(i, end=' ')\n",
    "\n",
    "    def print_clusters(self, dataset, results):\n",
    "        for i in range(len(results)):\n",
    "            cluster = results[i]\n",
    "            print(\"\\nCluster \" + str(i) + \"(\" + str(len(cluster)) + \" items):\")\n",
    "            for item in cluster:\n",
    "                self.__print_index(dataset, item)\n",
    "        print(\"\\n\")\n",
    "\n",
    "    def __item_in_cluster(self, results, constraint_pair):\n",
    "        for i in range(len(results)):\n",
    "            cluster = list(results[i])\n",
    "            res = 0\n",
    "            for i, item in enumerate(cluster):\n",
    "                if((item==constraint_pair[0]).all() or (item==constraint_pair[1]).all()):\n",
    "                    res+=1\n",
    "            if(res==2):\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def __compute_evaluation_must_link(self, n_ml, a):\n",
    "        return a/float(n_ml)\n",
    "\n",
    "    def __compute_evaluation_cannot_link(self, n_cl, b):\n",
    "        return b/float(n_cl)\n",
    "\n",
    "    def __compute_evaluation_ordinary(self, n, a, b):\n",
    "        return (a + b)/float(n)\n",
    "\n",
    "    def __compute_evaluation_overall(self, n, a, b):\n",
    "        return ((a + b)/((n*(float(n)-1))/2))\n",
    "\n",
    "    def print_evaluation(self, dataset, results, must_link, cannot_link):\n",
    "        n_ml = len(must_link)\n",
    "        n_cl = len(cannot_link)\n",
    "        n = n_ml + n_cl\n",
    "        \n",
    "        a=0\n",
    "        for i in range(len(must_link)):\n",
    "            constraint = must_link[i]\n",
    "            if(self.__item_in_cluster(results,constraint)):\n",
    "                a+=1\n",
    "        b=0\n",
    "        for i in range(len(cannot_link)):\n",
    "            constraint = cannot_link[i]\n",
    "            if(not self.__item_in_cluster(results,constraint)):\n",
    "                b+=1\n",
    "        \n",
    "        evaluation_must_link = self.__compute_evaluation_must_link(n_ml, a)\n",
    "        evaluation_cannot_link = self.__compute_evaluation_cannot_link(n_cl, b)\n",
    "        evaluation_ordinary = self.__compute_evaluation_ordinary(n, a, b)\n",
    "        evaluation_overall = self.__compute_evaluation_overall(n, a, b)\n",
    "\n",
    "        print(\"n=\" + str(n))\n",
    "        print(\"a=\" + str(a))\n",
    "        print(\"b=\" + str(b))\n",
    "        print(\"evaluation_must_link=\" + str(evaluation_must_link))\n",
    "        print(\"evaluation_cannot_link=\" + str(evaluation_cannot_link))\n",
    "        print(\"evaluation_ordinary=\" + str(evaluation_ordinary))\n",
    "        print(\"evaluation_overall=\" + str(evaluation_overall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'dataProcessor'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-2793ab0f3924>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgenfromtxt\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mrand\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdataProcessor\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mProcessor\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconstrainedKMeans\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConstrainedKMeans\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDistanceMetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReportResults\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'dataProcessor'"
     ]
    }
   ],
   "source": [
    "import numpy as np;\n",
    "from numpy import genfromtxt;\n",
    "import random as rand;\n",
    "from dataProcessor import Processor;\n",
    "from constrainedKMeans import ConstrainedKMeans, DistanceMetrics, ReportResults;\n",
    "\n",
    "# Dataset loading\n",
    "print('getting dataset...')\n",
    "dataset_raw = genfromtxt('test_data', delimiter=',', dtype=None) # numeric only dataset\n",
    "print('Got dataset!');\n",
    "\n",
    "# Data pre-processing\n",
    "print('Starting data processing...');\n",
    "processor = Processor(dataset_raw)\n",
    "processor.apply_scaling() # just in case the whole dataset is composed by numeric attributes\n",
    "dataset = processor.get_data()\n",
    "\n",
    "# Algorithm execution\n",
    "k_clusters = 4 # number of classes from the original dataset\n",
    "converge_threshold = 0.02\n",
    "distance_metric = DistanceMetrics.EuclidianDistance() # just in case the whole dataset is composed by numeric attributes\n",
    "constrained_kmeans = ConstrainedKMeans(k_clusters, converge_threshold, distance_metric)\n",
    "\n",
    "must_link = [\n",
    "    [dataset[0], dataset[1]],\n",
    "    [dataset[1], dataset[2]],\n",
    "\t[dataset[2], dataset[12]],\n",
    "]\n",
    "cannot_link = [\n",
    "\t[dataset[10], dataset[11]]\n",
    "]\n",
    "results = constrained_kmeans.clusterize(dataset, must_link, cannot_link)\n",
    "\n",
    "# Results report\n",
    "report = ReportResults()\n",
    "report.print_clusters(dataset, results)\n",
    "\n",
    "# Evaluation\n",
    "report.print_evaluation(dataset, results, must_link, cannot_link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
