{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "name": "tf_RNN_tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "BEpDgyZ7PYlt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "outputId": "4d029dc8-1c05-47a4-aeb1-94343dbbc028"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "\n",
        "# seqeunce.take(5) 문법 변경 문제 해결\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "# 세익스피어 파일 다운로드\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', \n",
        "                                       'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt\n",
            "1122304/1115394 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59QOcxdJPYl2",
        "colab_type": "code",
        "outputId": "dc4fcec2-9f5e-4a34-d9a9-8ad6bfd42652",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 읽은 다음 파이썬 2와 호환되도록 디코딩합니다.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# 텍스트의 길이는 그 안에 있는 문자의 수입니다.\n",
        "print ('텍스트의 길이: {}자'.format(len(text)))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "텍스트의 길이: 1115394자\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8qzRwMkPYl6",
        "colab_type": "code",
        "outputId": "a69d3d60-75c8-4ce2-dd13-2e3989e21f30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "print(text[:250])"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sRin30g6PYl-",
        "colab_type": "code",
        "outputId": "e9b96398-cddd-43c4-f11b-b92a2ddae313",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 파일의 고유 문자수를 출력합니다.\n",
        "vocab = sorted(set(text))\n",
        "print ('고유 문자수 {}개'.format(len(vocab)))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "고유 문자수 65개\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOf5AT55PYmB",
        "colab_type": "code",
        "outputId": "1cef86d6-5e92-4f31-9c02-0d7802df6c88",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# 고유 문자에서 인덱스로 매핑 생성\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\n",
            "  '\\n':   0,\n",
            "  ' ' :   1,\n",
            "  '!' :   2,\n",
            "  '$' :   3,\n",
            "  '&' :   4,\n",
            "  \"'\" :   5,\n",
            "  ',' :   6,\n",
            "  '-' :   7,\n",
            "  '.' :   8,\n",
            "  '3' :   9,\n",
            "  ':' :  10,\n",
            "  ';' :  11,\n",
            "  '?' :  12,\n",
            "  'A' :  13,\n",
            "  'B' :  14,\n",
            "  'C' :  15,\n",
            "  'D' :  16,\n",
            "  'E' :  17,\n",
            "  'F' :  18,\n",
            "  'G' :  19,\n",
            "  ...\n",
            "}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zucz0v80PYmE",
        "colab_type": "code",
        "outputId": "77074ae3-1e3c-4507-81bc-e1e746e4b20d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 텍스트에서 처음 13개의 문자가 숫자로 어떻게 매핑되었는지를 보여줍니다\n",
        "print ('{} ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen' ---- 문자들이 다음의 정수로 매핑되었습니다 ---- > [18 47 56 57 58  1 15 47 58 47 64 43 52]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fU7JYJk8PYmH",
        "colab_type": "code",
        "outputId": "b2df5341-5dd4-44a4-ff1a-fd7668964723",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# 단일 입력에 대해 원하는 문장의 최대 길이\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//seq_length\n",
        "\n",
        "# 훈련 샘플/타깃 만들기 / 원하는 만큼 잘라낼 수 있는 함수\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDJbVYaPPYmK",
        "colab_type": "code",
        "outputId": "684df109-e3d0-4390-84bb-4f0720bb968e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# batch 메서드는 이 개별 문자들을 원하는 크기의 시퀀스로 쉽게 변환할 수 있습니다.\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwQfOXlFPYmO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train / test split\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYOfVy9yPYmR",
        "colab_type": "code",
        "outputId": "db022165-7dbe-4a94-a7ed-5d2d1d1732e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "for input_example, target_example in  dataset.take(2):\n",
        "  print ('입력 데이터: ', repr(''.join(idx2char[input_example.numpy()])))\n",
        "  print ('타깃 데이터: ', repr(''.join(idx2char[target_example.numpy()])))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력 데이터:  'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "타깃 데이터:  'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "입력 데이터:  'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you '\n",
            "타깃 데이터:  're all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_A3sjQGvPYmU",
        "colab_type": "code",
        "outputId": "d8205146-13ac-464c-a242-0568d6395c6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "# 이 벡터의 각 인덱스는 하나의 타임 스텝(time step)으로 처리됩니다. \n",
        "# 타임 스텝 0의 입력으로 모델은 \"F\"의 인덱스를 받고 다음 문자로 \"i\"의 인덱스를 예측합니다. \n",
        "# 다음 타임 스텝에서도 같은 일을 하지만 RNN은 현재 입력 문자 외에 이전 타임 스텝의 컨텍스트(context)를 고려합니다.\n",
        "\n",
        "for i, (input_idx, target_idx) in enumerate(zip(input_example[:5], target_example[:5])):\n",
        "    print(\"{:4d}단계\".format(i))\n",
        "    print(\"  입력: {} ({:s})\".format(input_idx, repr(idx2char[input_idx])))\n",
        "    print(\"  예상 출력: {} ({:s})\".format(target_idx, repr(idx2char[target_idx])))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   0단계\n",
            "  입력: 39 ('a')\n",
            "  예상 출력: 56 ('r')\n",
            "   1단계\n",
            "  입력: 56 ('r')\n",
            "  예상 출력: 43 ('e')\n",
            "   2단계\n",
            "  입력: 43 ('e')\n",
            "  예상 출력: 1 (' ')\n",
            "   3단계\n",
            "  입력: 1 (' ')\n",
            "  예상 출력: 39 ('a')\n",
            "   4단계\n",
            "  입력: 39 ('a')\n",
            "  예상 출력: 50 ('l')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-u1ZmBqPYmY",
        "colab_type": "code",
        "outputId": "3a3c1e5e-fbff-441c-d68d-f9937f5cb5db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 텍스트를 다루기 쉬운 시퀀스로 분리하기 위해 tf.data를 사용했습니다. \n",
        "# 그러나 이 데이터를 모델에 넣기 전에 데이터를 섞은 후 배치를 만들어야 합니다.\n",
        "\n",
        "# 배치 크기\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# 데이터셋을 섞을 버퍼 크기\n",
        "# (TF 데이터는 무한한 시퀀스와 함께 작동이 가능하도록 설계되었으며,\n",
        "# 따라서 전체 시퀀스를 메모리에 섞지 않습니다. 대신에,\n",
        "# 요소를 섞는 버퍼를 유지합니다).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UDnJlD00PYmb",
        "colab_type": "text"
      },
      "source": [
        "## 모델 설계\n",
        "\n",
        "모델을 정의하려면 tf.keras.Sequential을 사용합니다. 이 간단한 예제에서는 3개의 층을 사용하여 모델을 정의합니다:\n",
        "\n",
        "- tf.keras.layers.Embedding : 입력층. embedding_dim 차원 벡터에 각 문자의 정수 코드를 매핑하는 훈련 가능한 검색 테이블.\n",
        "- tf.keras.layers.GRU : 크기가 units = rnn_units인 RNN의 유형(여기서 LSTM층을 사용할 수도 있습니다.\n",
        "- tf.keras.layers.Dense : 크기가 vocab_size인 출력을 생성하는 출력층."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9BMZEGNMPYmd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 문자로 된 어휘 사전의 크기\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# 임베딩 차원\n",
        "embedding_dim = 256\n",
        "\n",
        "# RNN 유닛(unit) 개수\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GAfDX9H5PYmg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, \n",
        "                              embedding_dim,\n",
        "                              batch_input_shape = [batch_size, None]),\n",
        "    tf.keras.layers.LSTM(rnn_units,\n",
        "                        return_sequences = True,\n",
        "                        stateful = True,\n",
        "                        recurrent_initializer = 'glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZwpR465tPYmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = len(vocab),\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "8P8_hNXAPYml",
        "colab_type": "code",
        "outputId": "c4623586-ab51-4b63-e029-3ff278e6aa15",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xa2eTuVaS0nf",
        "colab_type": "code",
        "outputId": "6a25c391-4180-4a72-b146-98cb6c84a256",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 544
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "print(device_lib.list_local_devices())"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[name: \"/device:CPU:0\"\n",
            "device_type: \"CPU\"\n",
            "memory_limit: 268435456\n",
            "locality {\n",
            "}\n",
            "incarnation: 2568338510252030418\n",
            ", name: \"/device:XLA_CPU:0\"\n",
            "device_type: \"XLA_CPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 14234868150382821748\n",
            "physical_device_desc: \"device: XLA_CPU device\"\n",
            ", name: \"/device:XLA_GPU:0\"\n",
            "device_type: \"XLA_GPU\"\n",
            "memory_limit: 17179869184\n",
            "locality {\n",
            "}\n",
            "incarnation: 7989945687532680196\n",
            "physical_device_desc: \"device: XLA_GPU device\"\n",
            ", name: \"/device:GPU:0\"\n",
            "device_type: \"GPU\"\n",
            "memory_limit: 15956161332\n",
            "locality {\n",
            "  bus_id: 1\n",
            "  links {\n",
            "  }\n",
            "}\n",
            "incarnation: 14635948153634449088\n",
            "physical_device_desc: \"device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0\"\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "_W1EyzmBPYmo",
        "colab_type": "code",
        "outputId": "550d3f66-e4e0-4937-a6b8-b20589fc77af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "#   print(example_batch_predictions)\n",
        "  print(example_batch_predictions.shape, \"# (배치 크기, 시퀀스 길이, 어휘 사전 크기)\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(64, 100, 65) # (배치 크기, 시퀀스 길이, 어휘 사전 크기)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPzHXpOkPYmr",
        "colab_type": "code",
        "outputId": "ba00a0e3-35c1-43fb-e977-7fb054252d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "print(dataset.take(1))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<DatasetV1Adapter shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SoDdhkzJPYmu",
        "colab_type": "code",
        "outputId": "34c66545-4bff-44cf-d920-8bc8db87ebef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "print(sampled_indices)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[44 24 56 47  2 17 47 57 28 42  1  9 27 58 29 43 15 38 52 30  8  7 56 41\n",
            " 39 22 10 59 49 36 21 27 22 30 19 37 55 54 64  8 15 32 56 18 20 29 39 16\n",
            "  7 38  8 15 43 35 55  6 58 38 45  0  6  8 13 44 41  1  1 36 30 33 27 26\n",
            "  2  9 58 58 28 35 27 60 55 53 33 64 51 62 61 26 40 10 42 28 55 59 58 47\n",
            " 48 38 17 41]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VAp5BwE9PYmx",
        "colab_type": "code",
        "outputId": "1b7ebbc7-78df-46f0-917d-5d0c7f4e06b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# pretrained model prediction result\n",
        "\n",
        "print(\"입력: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"예측된 다음 문자: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력: \n",
            " 't:\\nAs mine on hers, so hers is set on mine;\\nAnd all combined, save what thou must combine\\nBy holy ma'\n",
            "\n",
            "예측된 다음 문자: \n",
            " 'fLri!EisPd 3OtQeCZnR.-rcaJ:ukXIOJRGYqpz.CTrFHQaD-Z.CeWq,tZg\\n,.Afc  XRUON!3ttPWOvqoUzmxwNb:dPqutijZEc'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz1t2TgCPYm1",
        "colab_type": "code",
        "outputId": "124bb712-b00d-47f9-a71d-32fecb27b7bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# 손실함수 정의\n",
        "# 출력값이 로짓의 형태이기 때문에 from_logit = True\n",
        "\n",
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"예측 배열 크기(shape): \", example_batch_predictions.shape, \" # (배치 크기, 시퀀스 길이, 어휘 사전 크기\")\n",
        "print(\"스칼라 손실:          \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "예측 배열 크기(shape):  (64, 100, 65)  # (배치 크기, 시퀀스 길이, 어휘 사전 크기\n",
            "스칼라 손실:           4.173877\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYnuZpzBPYm6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tf.keras.Model.compile 메서드를 사용하여 훈련 절차를 설정합니다. \n",
        "# 기본 매개변수의 tf.keras.optimizers.Adam과 손실 함수를 사용합니다.\n",
        "\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niQu-JiXPYnA",
        "colab_type": "code",
        "outputId": "416fa965-aa3b-43db-fc9c-560b27cad925",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_2 (Embedding)      (64, None, 256)           16640     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (64, None, 1024)          5246976   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (64, None, 65)            66625     \n",
            "=================================================================\n",
            "Total params: 5,330,241\n",
            "Trainable params: 5,330,241\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rx244H5pPYnD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#체크포인트 구성\n",
        "# tf.keras.callbacks.ModelCheckpoint를 사용하여 훈련 중 체크포인트(checkpoint)가 저장되도록 합니다.\n",
        "\n",
        "# 체크포인트가 저장될 디렉토리\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# 체크포인트 파일 이름\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "upSy7rXDPYnG",
        "colab_type": "code",
        "outputId": "d84a37b6-17b0-483a-87bf-f3c74ec59650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "172/172 [==============================] - 38s 218ms/step - loss: 2.7615\n",
            "Epoch 2/10\n",
            "172/172 [==============================] - 36s 209ms/step - loss: 2.0268\n",
            "Epoch 3/10\n",
            "172/172 [==============================] - 36s 208ms/step - loss: 1.7610\n",
            "Epoch 4/10\n",
            "172/172 [==============================] - 35s 206ms/step - loss: 1.6101\n",
            "Epoch 5/10\n",
            "172/172 [==============================] - 36s 206ms/step - loss: 1.5132\n",
            "Epoch 6/10\n",
            "172/172 [==============================] - 35s 206ms/step - loss: 1.4470\n",
            "Epoch 7/10\n",
            "172/172 [==============================] - 35s 206ms/step - loss: 1.3970\n",
            "Epoch 8/10\n",
            "172/172 [==============================] - 35s 206ms/step - loss: 1.3567\n",
            "Epoch 9/10\n",
            "172/172 [==============================] - 35s 206ms/step - loss: 1.3220\n",
            "Epoch 10/10\n",
            "172/172 [==============================] - 35s 205ms/step - loss: 1.2886\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MpT4gUU0PYnK",
        "colab_type": "code",
        "outputId": "bc676ad1-ef7e-4a34-8c3a-f0274f997aa8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWduGXH4eJE9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nc2ij5EjeLH0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # 평가 단계 (학습된 모델을 사용하여 텍스트 생성)\n",
        "\n",
        "  # 생성할 문자의 수\n",
        "  num_generate = 1000\n",
        "\n",
        "  # 시작 문자열을 숫자로 변환(벡터화)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # 결과를 저장할 빈 문자열\n",
        "  text_generated = []\n",
        "\n",
        "  # 온도가 낮으면 더 예측 가능한 텍스트가 됩니다.\n",
        "  # 온도가 높으면 더 의외의 텍스트가 됩니다.\n",
        "  # 최적의 세팅을 찾기 위한 실험\n",
        "  temperature = 1.0\n",
        "\n",
        "  # 여기에서 배치 크기 == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # 배치 차원 제거\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # 범주형 분포를 사용하여 모델에서 리턴한 단어 예측\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # 예측된 단어를 다음 입력으로 모델에 전달\n",
        "      # 이전 은닉 상태와 함께\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjQQH294eP7d",
        "colab_type": "code",
        "outputId": "9bc49289-b314-4cfb-e1e7-7dbc94e6062f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO: $XV$JJK&Xj&YMPX$xD&M&k&WJXGVJX&JVVj$J&PJ&jXXQDjQ$&P3XQX3xzMXjJjQ3P&XSQXGVK&KQq&$Q&&J&KzQxXjSM&QzXDqV&V$xDxjQ$H&QjxXz$VYMDqxVVzQAGQX&XVzDzYQ3G&DY&z&xXXV&zVjDzjjVzQzDVX&QaXXXQ$&z&VXX&UK$KJjp&$V&DJXQX&JxXVJXLMKYY$SY3&Vj$VzMJXX&&JkzXJQPYJX$KQXjB3$&XXMMj$$JjKV&XQKVDQqjKzKVJ&QxjQMQGj&&xzqQkM3XNVX3jJX&DDJ$&&&PMxjjXQQQXXQJjYQgX3JDjVX&$xVz&&&JZXXXjQjMx&$&Q3VY&JVXj&&XXxqz&DKjZzjGJX&&xx3XV&XxXXqXKKV3JjMY&XzVQVjYKDGMMzXxJzDYQJjzXSYQZqM&MXX$&QqX&NzjX&j3X&QPQXxG$QXXQxkQ&jVJMSYSzJ&Q&XQMjN&X&KjJD3kj$VV3&jCMCpBXXjXMVjYYX&Pz&$Q$QMXX$zJ3X&MM$jz-SGXjKVQjQMVJX?JXBJj$PXJj&Nq&jj$jMNX&X$S&3JXV&V&XPJQ3$XzV3jj&jJ&XQXV&&&KVQQxXjMxXHxMX&KKMxVxMxxzjVXjj&YDX$Q&zH&xjMMXxJzLjXqXXMjM$GO3GXG3QjVUUYV&3&EXD&QXMDXXzz&$jXxNz&Qzj$qXXX3&xDxzKzVzjQ$MMYXXjQX$&&XDz3DJCVz$VX3XQFzjXXkMXJMQX3XjKKYJKX&&zzYQjXXKDVX&3&GzVJ&YXIXTxzVxjYVKxDYDYX$zCzMzXMX$xQ3JJCQ&XKJZJPQXJJMXLXz3Dz3JXGxjF&Kj$MM&&3&jYXQMx$X3zNX&33jX&qjHCQXXjV$z&z$D&M$M&PJjXKQXX$&;XQDVxQ&&QjX&XzJjQX&XXXzQXXzYVVjJX&QQJgJXqXxX&XJqX&HxVKzQXXjVGX&$&&$jVzQxPXXXJXGCSxDzDJjQXXKB3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "03hZAoxReSh1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}